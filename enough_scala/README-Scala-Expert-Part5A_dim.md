# üìä Scala Expert ‚Äî Partie 5A : Dimensionnement Spark (Sizing complet)

**But** : dimensionner correctement un job Spark (CPU, RAM, partitions, shuffle) pour tenir un **SLA** au **co√ªt minimal**.

---

## 1) Variables √† estimer avant tout
- **Taille d‚Äôentr√©e** (brut / compress√©) et **format** (Parquet/CSV/JSON)
- **Facteur d‚Äôexpansion m√©moire** apr√®s d√©codage (‚âà √ó2 √† √ó6 selon colonnes)
- **Shuffle attendu** (joins, aggregations) ‚Üí nombre d‚Äô√©tapes et taille
- **SLA** : ex. ‚Äúfinir en ‚â§ 30 min‚Äù (batch) ou ‚Äúlatence ‚â§ 60 s‚Äù (streaming)
- **Infra** : vCores/RAM par n≈ìud, I/O (HDFS/S3), r√©seau, budget

---

## 2) R√®gles rapides (cheat‚Äësheet)
- **C≈ìurs/ex√©cuteur** : **4 √† 6** (sweet‚Äëspot)
- **RAM Heap/ex√©cuteur** : **16‚Äì32 Go** (√©viter > 64 Go)
- **Overhead JVM** : **7‚Äì10 %** (`spark.executor.memoryOverhead`)
- **Parallelisme** : **2‚Äì3 partitions / c≈ìur** disponible
- **Partitions d‚Äôentr√©e (Parquet)** : **128‚Äì256 Mo** par partition
- **Shuffle partitions** : **200‚Äì600** pour commencer (`spark.sql.shuffle.partitions`)
- **Broadcast join** si dimension ‚â§ **256‚Äì512 Mo** (`autoBroadcastJoinThreshold`)
- **Dynamic allocation** : ON si cluster partag√© ; OFF si SLA strict & workload stable

---

## 3) M√©thode ‚Äúworksheet‚Äù (formules pratiques)
1) **C≈ìurs totaux**
```
total_cores = executors * cores_per_executor
parallelism_cible ‚âà 2‚Äì3 √ó total_cores
```
2) **Partitions**
- Entr√©e Parquet : `nb_partitions_in = ceil(size_in_bytes / 256MB)`
- Apr√®s filtres/agg : **repartition** pour rester ‚âà `2‚Äì3 √ó total_cores`
3) **M√©moire/ex√©cuteur**
```
executor_heap   ‚âà working_set_per_core √ó cores_per_executor
overhead        ‚âà 7‚Äì10 % de executor_heap
executor_total  = executor_heap + overhead
```
4) **Nombre d‚Äôex√©cuteurs**
- Batch : √† partir du **SLA** + d√©bit I/O ; viser 70‚Äì80 % d‚Äôutilisation cluster
- Streaming : dimensionner sur le **d√©bit cr√™te** (events/s) √ó **co√ªt/√©v√©nement**

---

## 4) Exemples chiffr√©s
### 4.1 ‚Äî Batch Parquet **1 To**, SLA **30 min**
- Entr√©e : 1 To Parquet (S3/HDFS), 2 aggregations + 1 join
- Expansion m√©moire estim√©e √ó3 ‚Üí 3 To **r√©partis** sur le cluster

**Plan recommand√©**
- `cores/executor = 5`, `executor_heap = 24g`, `overhead = 3g` ‚Üí **~27g total**
- **40 ex√©cuteurs** ‚Üí **200 c≈ìurs** totaux
- **Shuffle partitions = 600** (‚âà 3 √ó c≈ìurs)

**Params initiaux**
```
spark.executor.cores=5
spark.executor.memory=24g
spark.executor.memoryOverhead=3g
spark.dynamicAllocation.enabled=false
spark.sql.shuffle.partitions=600
spark.sql.autoBroadcastJoinThreshold=256MB
spark.sql.adaptive.enabled=true
spark.sql.parquet.filterPushdown=true
spark.sql.files.maxPartitionBytes=268435456  # ~256MB
```

### 4.2 ‚Äî Streaming **50k events/s**, fen√™tre **10 min**, SLA **60 s**
- Fen√™tre 10 min ‚Üí **30 M events** en vol max (ordre de grandeur)
- Pipeline : parse JSON + enrichissement + aggregate(key)

**Plan recommand√©**
- `cores/executor = 4`, `executor_heap = 16g`, `overhead = 2g`
- **24 ex√©cuteurs** ‚Üí **96 c≈ìurs** totaux ; trigger **5 s**

**Params initiaux**
```
spark.executor.cores=4
spark.executor.memory=16g
spark.executor.memoryOverhead=2g
spark.streaming.backpressure.enabled=true
spark.sql.shuffle.partitions=400
spark.sql.adaptive.enabled=true
spark.sql.streaming.stateStore.maintenanceInterval=60s
```
**Astuces** : surveiller **state size** (Spark UI), **watermarks** pour purge, backpressure ON.

---

## 5) M√©moire & stockage : ce qu‚Äôil faut compter
- **Heap** (RDD/DataFrame, encoders, objets)
- **Overhead/off‚Äëheap** (buffers shuffle, s√©rialisation)
- **Shuffle files** (disque local/r√©seau)
- **Broadcast** (r√©pliqu√© par ex√©cuteur)

**Raccourcis**
```
executor_total ‚âà executor_heap √ó 1.1
cluster_RAM    ‚âà executor_total √ó executors
```

---

## 6) Partitions & Shuffle
- Entr√©e Parquet : **128‚Äì256 Mo**/partition
- Garder **2‚Äì3√ó c≈ìurs** tout au long du pipeline (repartition/coalesce)
- **Skew** (cl√©s chaudes) : **salting** + **AQE** (`spark.sql.adaptive.skewJoin.enabled=true`)
- **Broadcast join** si dim ‚â§ **256‚Äì512 Mo** ; sinon √©viter le shuffle massif

---

## 7) I/O, formats & r√©seau
- **Parquet** ‚â´ CSV/JSON (colonnaire, compression, predicate pushdown)
- **S3** : latence, privil√©gier gros blocs, **coalesce** en sortie
- **HDFS** : v√©rifier r√©plication & throughput par datanode

---

## 8) Dynamic allocation & autoscaling
**ON** (cluster mutualis√©, workloads variables)
```
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=80
spark.dynamicAllocation.initialExecutors=10
```
**OFF** (SLA strict & stable) ‚Üí pr√©dictible, plus simple √† profiler

---

## 9) Anti‚Äëpatterns & pi√®ges (et comment les √©viter)
- Ex√©cuteurs **trop gros** ‚Üí GC & contention : rester sur **4‚Äì6 c≈ìurs**, **16‚Äì32 Go heap**
- Trop peu de partitions ‚Üí CPU sous‚Äëutilis√©s ; trop ‚Üí overhead & shuffle gargantuesque
- **UDF** partout ‚Üí pr√©f√©rer fonctions SQL natives (Catalyst)
- **Nulls** non g√©r√©s ‚Üí `Option`/valeurs par d√©faut + sch√©mas stricts
- Ignorer Spark UI ‚Üí toujours profiler **stages**, **tasks**, **skew**, **spilled**

---

## 10) Tableaux de tuning (r√©cap express)
| √âl√©ment | Recommandation de d√©part |
|---|---|
| C≈ìurs/ex√©cuteur | 4‚Äì6 |
| Heap/ex√©cuteur | 16‚Äì32 Go |
| Overhead | 7‚Äì10 % de la heap |
| Partitions/parall√©lisme | 2‚Äì3 √ó c≈ìurs totaux |
| Parquet partition | 128‚Äì256 Mo |
| Shuffle partitions | 200‚Äì600 |
| Broadcast join | ‚â§ 256‚Äì512 Mo |
| Dynamic allocation | ON (mutualis√©), OFF (SLA strict) |

---

**Fin ‚Äî Partie 5A**. Prochaine : **Partie 5B ‚Äî Mise en production Spark** (templates YAML/JSON, monitoring, check‚Äëlist).
